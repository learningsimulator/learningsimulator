#+title: The Tolman Mechanism
#+subtitle: Development Notes
#+author: Stefano Ghirlanda
#+options: toc:nil ':t
#+latex_header: \usepackage{cleveref}
#+latex_header: \hypersetup{hidelinks=true}

* Rationale

- The Tolman mechanism implements a learning mechanism that formalizes
  common theorizing in animal cognition.
- The animal is assumed to build a mental model of its environment.
- The model is then used to plan how to reach states with high value.
- There are major computational challenges in such a mechanism and it
  is unlikely that animals (or even people) can apply this strategy
  in cases beyond a certain complexity.
- We restrict the mechanism to work in worlds with a single goal, and
  that "restart" after the goal is obtained.
 
* Variables

- The Tolman mechanism learns \(S\to B\to S'\) transition
  probabilities.
- These estimates are called \(z(S,B,S')\).
- Everything else is decided when responding.

* Learning 

- There is a single learning rate, \(\alpha_z\).

- Imagine first that all stimuli are made of a single element. When
  \(S\to\ B\to S'\) is observed, \(z(S,B,S'')\) is updated for all
  \(S''\) as follows:
  #+begin_export latex
  \begin{equation}
    \label{eq:sbs-update}
    \Delta z(S,B,S'') = \alpha_z \left( \lambda_{S''} - z(S,B,S'') \right)
  \end{equation}
  #+end_export
  where $\lambda_{S''}=1$ when $S''=S'$ (the state that actually
  occurred), and 0 for all other states.

- With sufficient experience and sufficiently small $\alpha_z$,
  cref:eq:sbs-update will converge to the actual transition
  probabilities.

- How to extend to stimuli with more elements? We can use
  #+begin_export latex
  \begin{equation}
    \label{eq:sbs-multi}
    z(S_1 \ldots S_n,B,S') = \max\left( \sum_{i=1}^n z(S_i,B,S'), 1 \right)
  \end{equation}
  #+end_export
  and then update each $z(S_i,B,S')$ using cref:eq:sbs-update. The
  $\max$ operation takes into account that these are probabilities. In
  this way, different elements compete for accruing $z$ value in the
  same way as they would compete for accruing $v$ or $w$ values in
  other mechanisms.

- It becomes complicated to also decompose $S'$ into elements. It also
  makes less sense.

* Responding

- Responding assumes that the world model in $z$ is true.

- Responding should then make the "best plan" given this
  knowledge.

- Evaluating all possible plans is tricky in general. Let's work with
  a particular kind of world to begin with:
  1. There is only one valued stimulus, called the goal.
  2. When the goal is reached, the next state is determined by the
     world using a fixed rule (deterministic or stochastic). 

- The second condition means that we need to consider just how to
  reach the goal once, since after that the world "resets" to a
  statistically equivalent state.

- The first step is to find all paths to the goal. We start from the
  goal and we go backwards along possible transitions, that is, all
  transitions with $z>0$ for some $B$. If we eventually reach the
  current state, we store the sequence of transitions and call it a
  "path." 

- We calculate the expected /rate/ of return for all paths to the
  goal, that is the expected value divided the path length (number of
  actions):
#+begin_export latex
\begin{equation}
  \label{eq:path-value}
  v(\mathrm{path}) = \frac{ u(S_\mathrm{goal}) -\sum_{B\in\mathrm{path}} c(B)}{ l_\mathrm{path} } \prod_{S'\in\mathrm{path}} \Pr(S\to B\to S') 
\end{equation}
#+end_export

- In cref:eq:path-value, the sum is over all behaviors in the path,
  $l_\mathrm{path}$ is path length, and the product is over all
  transition probabilities in the path. (This is the probability that
  the plan will succeed.)

- We then give a value to each behavior that is feasible in response
  to the current stimulus. If the behavior is the first step on a path
  to the goal, its value is the $v$ value of the goal. If it is not,
  its value is ~start_v~.

- If a behavior is the first on more than one path, we can average the
  $v$'s using the success probability of each path, so we should store
  these somewhere.

- We then use softmax to choose a behavior.


